{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - ID 203459011\n",
    "All previous instructions hold. In addition, if you are using GPU, you must check that your code also runs on a CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:56:18.183984Z",
     "start_time": "2022-11-05T08:56:17.364249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import zipfile\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (40 points)\n",
    "\n",
    "Understanding and implementing the vanilla RNN cell. As you learned in class, the RNN has a certain structure that allows it to accept the previous hidden state the current input, and output an hidden state and an output vector. The RNN cell uses the same weights for all time steps, much like convolution uses the same weights for all the batches in the image. Even though you already are familiar with PyTorch, implementing the RNN you make sure you understand how this pivotal architecture works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:56:21.147631Z",
     "start_time": "2022-11-05T08:56:21.052521Z"
    }
   },
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN: step forward (10 points)\n",
    "\n",
    "First implement the function `rnn_step_forward` which implements the forward pass for a single timestep of a vanilla recurrent neural network. After doing so run the following to check your implementation. You should see errors less than 1e-7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:56:22.646868Z",
     "start_time": "2022-11-05T08:56:22.560505Z"
    }
   },
   "outputs": [],
   "source": [
    "def rnn_step_forward(x, prev_h, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh\n",
    "    activation function.\n",
    "\n",
    "    The input data has dimension D, the hidden state has dimension H, and we use\n",
    "    a minibatch size of N.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data for this timestep, of shape (N, D).\n",
    "    - prev_h: Hidden state from previous timestep, of shape (N, H)\n",
    "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - b: Biases of shape (H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - next_h: Next hidden state, of shape (N, H)\n",
    "    - cache: Tuple of values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    next_h, cache = None, None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement a single forward step for the vanilla RNN. Store the next  #\n",
    "    # hidden state and any values you need for the backward pass in the next_h   #\n",
    "    # and cache variables respectively.                                          #\n",
    "    ##############################################################################\n",
    "    xh = x.dot(Wx) # (N,D) * (D,H) = (N,H)\n",
    "    hh = prev_h.dot(Wh) # (N,H) * (H,H) = (N,H)\n",
    "    xh_hh = xh + hh # (N,H)\n",
    "    next_h = np.tanh(xh + hh + b) # (N,H)\n",
    "    \n",
    "    cache = (x, prev_h, Wx, Wh, b, next_h) # ((N,D) , (N,H), (D,H), ())\n",
    "\n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "    return next_h, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:56:23.305132Z",
     "start_time": "2022-11-05T08:56:23.235365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_h error:  6.292421426471037e-09\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 3, 10, 4\n",
    "\n",
    "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
    "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.2, 0.4, num=H)\n",
    "\n",
    "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "expected_next_h = np.asarray([\n",
    "  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
    "  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n",
    "  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n",
    "\n",
    "print('next_h error: ', rel_error(expected_next_h, next_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN: step backward (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:56:25.607946Z",
     "start_time": "2022-11-05T08:56:25.485294Z"
    }
   },
   "outputs": [],
   "source": [
    "def rnn_step_backward(dnext_h, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for a single timestep of a vanilla RNN.\n",
    "\n",
    "    Inputs:\n",
    "    - dnext_h: Gradient of loss with respect to next hidden state\n",
    "    - cache: Cache object from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradients of input data, of shape (N, D)\n",
    "    - dprev_h: Gradients of previous hidden state, of shape (N, H)\n",
    "    - dWx: Gradients of input-to-hidden weights, of shape (D, H)\n",
    "    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)\n",
    "    - db: Gradients of bias vector, of shape (H,)\n",
    "    \"\"\"\n",
    "    dx, dprev_h, dWx, dWh, db = None, None, None, None, None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement the backward pass for a single step of a vanilla RNN.      #\n",
    "    #                                                                            #\n",
    "    # HINT: For the tanh function, you can compute the local derivative in terms #\n",
    "    # of the output value from tanh.                                             #\n",
    "    ##############################################################################\n",
    "    x, prev_h, Wx, Wh, b, next_h = cache\n",
    "    d_common = (1 - next_h**2) * dnext_h # (N,H)\n",
    "    dx = d_common.dot(Wx.T) # (N,H) * (H,D) = (N,D)\n",
    "    dprev_h = d_common.dot(Wh.T) # (N,H) * (H,H) = (N,H)\n",
    "    dWx = x.T.dot(d_common) # (D,N) * (N,H) = (D,H)\n",
    "    dWh = prev_h.T.dot(d_common) # (H,N) * (N,H) = (H,H)\n",
    "    db = np.sum(d_common, axis=0) # (H,)\n",
    " \n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "    return dx, dprev_h, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:56:26.339352Z",
     "start_time": "2022-11-05T08:56:26.273571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  2.8730509825914645e-10\n",
      "dprev_h error:  1.931634610323001e-08\n",
      "dWx error:  6.586630396892865e-10\n",
      "dWh error:  9.131820369191576e-10\n",
      "db error:  8.569581669830872e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "N, D, H = 4, 5, 6\n",
    "x = np.random.randn(N, D)\n",
    "h = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
    "\n",
    "dnext_h = np.random.randn(*out.shape)\n",
    "\n",
    "fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fh = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n",
    "dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n",
    "\n",
    "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN: forward (10 points)\n",
    "Now that you have implemented the forward and backward passes for a single timestep of a vanilla RNN, you will combine these pieces to implement a RNN that process an entire sequence of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:56:27.753039Z",
     "start_time": "2022-11-05T08:56:27.690692Z"
    }
   },
   "outputs": [],
   "source": [
    "def rnn_forward(x, h0, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Run a vanilla RNN forward on an entire sequence of data. We assume an input\n",
    "    sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n",
    "    size of H, and we work over a minibatch containing N sequences. After running\n",
    "    the RNN forward, we return the hidden states for all timesteps.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data for the entire timeseries, of shape (N, T, D).\n",
    "    - h0: Initial hidden state, of shape (N, H)\n",
    "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - b: Biases of shape (H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - h: Hidden states for the entire timeseries, of shape (N, T, H).\n",
    "    - cache: Values needed in the backward pass\n",
    "    \"\"\"\n",
    "    h, cache = None, None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement forward pass for a vanilla RNN running on a sequence of    #\n",
    "    # input data. You should use the rnn_step_forward function that you defined  #\n",
    "    # above. You can use a for loop to help compute the forward pass.            #\n",
    "    ##############################################################################\n",
    "    N, T, D = x.shape\n",
    "    H = h0.shape[1]\n",
    "    h = np.zeros((N, T, H)) # save all hidden states\n",
    "    prev_h = h0\n",
    "    cache_list = [None] * T # cache for each time step\n",
    "    \n",
    "    for t in range(T):\n",
    "        prev_h, cache_t = rnn_step_forward(x[:,t,:], prev_h, Wx, Wh, b)\n",
    "        h[:,t,:] = prev_h\n",
    "        cache_list[t] = cache_t\n",
    "       \n",
    "    # save dimension information and cache from each timestamp for backward pass\n",
    "    cache = (N, T, D, H, cache_list)\n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "    return h, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:56:28.596746Z",
     "start_time": "2022-11-05T08:56:28.500371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h error:  7.728466151011529e-08\n"
     ]
    }
   ],
   "source": [
    "N, T, D, H = 2, 3, 4, 5\n",
    "\n",
    "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.7, 0.1, num=H)\n",
    "\n",
    "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
    "expected_h = np.asarray([\n",
    "  [\n",
    "    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n",
    "    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n",
    "    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n",
    "  ],\n",
    "  [\n",
    "    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n",
    "    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n",
    "    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n",
    "print('h error: ', rel_error(expected_h, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN: backward (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:56:30.427882Z",
     "start_time": "2022-11-05T08:56:30.367971Z"
    }
   },
   "outputs": [],
   "source": [
    "def rnn_backward(dh, cache):\n",
    "    \"\"\"\n",
    "    Compute the backward pass for a vanilla RNN over an entire sequence of data.\n",
    "\n",
    "    Inputs:\n",
    "    - dh: Upstream gradients of all hidden states, of shape (N, T, H)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of inputs, of shape (N, T, D)\n",
    "    - dh0: Gradient of initial hidden state, of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weights, of shape (D, H)\n",
    "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n",
    "    - db: Gradient of biases, of shape (H,)\n",
    "    \"\"\"\n",
    "    dx, dh0, dWx, dWh, db = None, None, None, None, None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement the backward pass for a vanilla RNN running an entire      #\n",
    "    # sequence of data. You should use the rnn_step_backward function that you   #\n",
    "    # defined above. You can use a for loop to help compute the backward pass.   #\n",
    "    ##############################################################################\n",
    "    N, T, D, H, cache_list = cache\n",
    "    dx = np.zeros((N, T, D))\n",
    "    dh_t = np.zeros((N, H))\n",
    "    dWx = np.zeros((D, H))\n",
    "    dWh = np.zeros((H, H))\n",
    "    dnext_h = np.zeros((N, H))\n",
    "    db = np.zeros((H,))\n",
    "    \n",
    "    for t in range(T-1, -1, -1):\n",
    "        dnext_h = dh[:,t,:] + dh_t\n",
    "        dx_t, dh_t, dWx_t, dWh_t, db_t = rnn_step_backward(dnext_h, cache_list[t])\n",
    "        dx[:,t,:] = dx_t\n",
    "        dWx += dWx_t\n",
    "        dWh += dWh_t\n",
    "        db += db_t\n",
    "    \n",
    "    dh0 = dh_t\n",
    "        \n",
    "    \n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "    return dx, dh0, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:56:31.205931Z",
     "start_time": "2022-11-05T08:56:31.110567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  2.5571085568563326e-09\n",
      "dh0 error:  5.423206039675471e-10\n",
      "dWx error:  3.579829809212802e-09\n",
      "dWh error:  7.994186410119177e-09\n",
      "db error:  5.755566528286172e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "\n",
    "fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:18:23.856501Z",
     "start_time": "2022-11-04T12:18:23.764777Z"
    }
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:18:23.856501Z",
     "start_time": "2022-11-04T12:18:23.764777Z"
    }
   },
   "source": [
    "##**Question:** \n",
    "When using valina RNN on long sequences, what could happen? What causes this to happen?   **(5 Points)**\n",
    "\n",
    "**Your answer:** First we can have Exploding / Vanishing Gradients due to h_0 is reflected each time we moving to the next time iteration.\n",
    "Second, when we have long sequences to train we need to memorize many parameters on the hidden layer which can be problamtic for computation and hardware capabilities. In this case we need to have many hidden states to memorize and run derivatives in the backprop as long as the sequence is larger, so computing long sequences is not so efficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:18:23.856501Z",
     "start_time": "2022-11-04T12:18:23.764777Z"
    }
   },
   "source": [
    "##**Question:** \n",
    "Could this problem be solved by a different model? How does it accomplish this?   **(5 Points)**\n",
    "\n",
    "**Your answer:** The memorization of hidden layer needed to been held in the generation process, can be splitted to smaller chunks. In this case we only need to derive the last hidden state to our next chunk. For that we can create batches of the hidden states layer that can help us prevent this problem. In this case we train and backprop each chunk from the hidden state alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Image Captioning in PyTorch (50 points)\n",
    "\n",
    "The goal of image captioning is to describe a given image using natural language. Using neural networks, we can partition the problem into two separate challenges. First, we need to extract meaningful features regarding the image that would help us describe it. Second, we need to generate a sequence of words that best fit those features. Luckily, the flexability of neural networks allows us to take a CNN architecture and connect it directly to a LSTM network. We only need to provide proper labels to train the new network we created. For this exercise, you will be provided with pretrained networks for both feature extraction and sentence generation, and you will connect the different components needed to make image captioning work.\n",
    "\n",
    "First, we define the feature extractor and the recurrent model seperately. The feature extractor takes an image and produces a vector representation of the image features. As those features hold information about the image, we will use that vector as the input for our recurrent model. The RNN will produce the image captioning using an LSTM architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:56:58.959886Z",
     "start_time": "2022-11-05T08:56:56.739008Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unzipping the pretrained models\n",
    "with zipfile.ZipFile(os.path.join('models', 'pretrained_model.zip'), 'r') as zip_ref:\n",
    "    zip_ref.extractall('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:56:59.211902Z",
     "start_time": "2022-11-05T08:56:59.120471Z"
    }
   },
   "outputs": [],
   "source": [
    "# unzipping vocabulary\n",
    "with zipfile.ZipFile(os.path.join('data', 'vocab.zip'), 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:57:00.957593Z",
     "start_time": "2022-11-05T08:57:00.879322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_path  ✓\n",
      "lstm_path  ✓\n",
      "vocab_path  ✓\n"
     ]
    }
   ],
   "source": [
    "conv_path = 'models/encoder-5-3000.pkl'\n",
    "lstm_path = 'models/decoder-5-3000.pkl'\n",
    "vocab_path   = 'data/vocab.pkl'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Use the following code to check if all the files are in place\n",
    "print(\"conv_path \", '✓' if os.path.isfile(conv_path) == True else '✗') \n",
    "print(\"lstm_path \", '✓' if os.path.isfile(lstm_path) == True else '✗') \n",
    "print(\"vocab_path \", '✓' if os.path.isfile(vocab_path) == True else '✗') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:24:56.821196Z",
     "start_time": "2022-11-04T12:24:56.758302Z"
    }
   },
   "source": [
    "## Implementing image captioning model **(40 points)**.\n",
    "\n",
    "As training a multimodal classifier could take some time and resources, we spared you the training phase this time. In this exercise, we use a pretrained model to solve the image captioning task. Using pretrained models is a common practice in the deep learning community and it's important to be aware of such techniques to save time and energy. In previous cells, we unzipped the necessary files, but in order to be able to load the models (and then use them) it is required to build the same PyTorch model as the pretrained model.\n",
    "\n",
    "**ConvNet architecture:** resnet152 (without last fc layer) -> fc layer -> BatchNorm1d\n",
    "\n",
    "**LSTM architecture:** LSTM -> linear -> embed\n",
    "\n",
    "We added more detailed instructions in the next cells, please make sure you follow them carefully. \n",
    "\n",
    "**Please make sure you construct your models based on the sizes we provided.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:57:02.311127Z",
     "start_time": "2022-11-05T08:57:02.251030Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_size   = 256      # dimension of word embedding vectors\n",
    "hidden_size  = 512      # dimension of lstm hidden states\n",
    "num_layers   = 1        # number of layers in lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:57:02.819615Z",
     "start_time": "2022-11-05T08:57:02.746612Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(ConvNet, self).__init__()\n",
    "        resnet = models.resnet152() # construct an nn.Sequential model without the last resnet152 layer\n",
    "        # make sure you define each of the None parameters\n",
    "        self.linear = None\n",
    "        self.bn = None\n",
    "        #############################################################################\n",
    "        # TO DO:                                                                    #\n",
    "        # Load the pretrained ResNet-152 network and replace the top fully          #\n",
    "        # connected layer, so we could pass the features of the network and not the #\n",
    "        # classification head which carries significantly less information.         #\n",
    "        # Afterwards, create a new sequential model which includes the resnet and   #\n",
    "        # add a new fully connected layer that outputs a vector with the size of    #\n",
    "        # the wanted embedding. Next, you should add a batchnorm layer with         #\n",
    "        # momentum=0.01 (BatchNorm1d parameter).                                    #\n",
    "        # This function has no return value.                                        #\n",
    "        #############################################################################\n",
    "\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # remove the last FC layer \n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    def forward(self, images):\n",
    "        #############################################################################\n",
    "        # TO DO:                                                                    #\n",
    "        # Define the forward propagation. You need to pass an image through the     #\n",
    "        # network and extract the feature vector. In this case, when using a        #\n",
    "        # perdefined network, you don't want to change it's weights.                #\n",
    "        # The rest of the layers you defined should accepts gradients for them to   #\n",
    "        # improve during training. Make sure you are inputing a correct shape       #\n",
    "        # to the batchnorm layer.                                                   #\n",
    "        # This function returns the features of the image.                          #\n",
    "        #############################################################################\n",
    "\n",
    "        features = self.resnet(images)\n",
    "        \n",
    "        # set features to the correct shape\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        features = self.linear(features)\n",
    "        features = self.bn(features)\n",
    "        \n",
    "        return features\n",
    "            \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.max_seg_length = max_seq_length\n",
    "        self.embed = None\n",
    "        self.lstm = None\n",
    "        self.linear = None\n",
    "        #############################################################################\n",
    "        # TO DO:                                                                    #\n",
    "        # Define the hyper-parameters and the layers of the pretrained LSTM.        #\n",
    "        # Create an Embedding layer that accepts the output of the                  #\n",
    "        # feature extractor.  Next, the built-in LSTM architecture in PyTorch.nn    #\n",
    "        # with the proper inputs (use the built-in documentation tool in Jupyter    #\n",
    "        # or just look at the official documentation online).                       #\n",
    "        # Define an additional linear layer that comes after the LSTM and outputs   #\n",
    "        # a vector that will support the size of our vocabulary.                    #\n",
    "        # This function has no return value.                                        #\n",
    "        #############################################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################           \n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        #############################################################################\n",
    "        # TO DO:                                                                    #\n",
    "        # Generate captions for a given image features.                             #\n",
    "        # First, obtain the output of the LSTM network.        #\n",
    "        # Next, use the hidden states to obtain the most probable word and store    #\n",
    "        # all the word predictions in the sampled_ids list. Don't forget to update  #\n",
    "        # the inputs for each timestep to continue making predictions based on the  #\n",
    "        # words you are alreaedy predicted.                                         #\n",
    "        # Make sure you keep track of the dimensions of the inputs and outputs,     #\n",
    "        # since PyTorch expects tensors with a batch dimension. You can use the     #\n",
    "        # methods .squeeze() and .unsqueeze()                                       #\n",
    "        # This function returns the list of predicted words.                        #\n",
    "        #############################################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################        \n",
    "\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:57:03.443192Z",
     "start_time": "2022-11-05T08:57:03.374504Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([224, 224], Image.Resampling.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:57:03.983209Z",
     "start_time": "2022-11-05T08:57:03.915967Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:57:04.551558Z",
     "start_time": "2022-11-05T08:57:04.432645Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:57:07.312013Z",
     "start_time": "2022-11-05T08:57:05.040860Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build models\n",
    "conv = ConvNet(embed_size).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
    "lstm = LSTM(embed_size, hidden_size, len(vocab), num_layers)\n",
    "conv = conv.to(device)\n",
    "lstm = lstm.to(device)\n",
    "\n",
    "# Load the trained model parameters\n",
    "conv.load_state_dict(torch.load(conv_path))\n",
    "lstm.load_state_dict(torch.load(lstm_path))\n",
    "\n",
    "# Prepare an image\n",
    "image_path = os.path.join('data', 'pic.jpg')\n",
    "image = load_image(image_path, transform)\n",
    "image_tensor = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T08:57:09.306853Z",
     "start_time": "2022-11-05T08:57:07.503361Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate an caption from the image\n",
    "feature = conv(image_tensor)\n",
    "sampled_ids = lstm.sample(feature)\n",
    "sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "\n",
    "# Convert word_ids to words\n",
    "sampled_caption = []\n",
    "for word_id in sampled_ids:\n",
    "    word = vocab.idx2word[word_id]\n",
    "    sampled_caption.append(word)\n",
    "    if word == '<end>':\n",
    "        break\n",
    "sentence = ' '.join(sampled_caption)\n",
    "\n",
    "# Print out the image and the generated caption\n",
    "print(sentence)\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(np.asarray(image));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:21:49.196557Z",
     "start_time": "2022-11-04T12:21:49.127510Z"
    }
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:21:49.196557Z",
     "start_time": "2022-11-04T12:21:49.127510Z"
    }
   },
   "source": [
    "##**Question:** \n",
    "Could you think of a method to evalute the goodness of the captioning model? **(5 Points)**\n",
    "\n",
    "**Your answer:** *Fill this in*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:21:49.196557Z",
     "start_time": "2022-11-04T12:21:49.127510Z"
    }
   },
   "source": [
    "##**Question:** \n",
    "Could you think of scenarios where the model fails? Why would that happen? **(5 Points)**\n",
    "\n",
    "We encourage you to test it on different images (jpg). \n",
    "\n",
    "**Your answer:** *Fill this in*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "370b431e1dc34a9ba742b938a84ab67b5373c19d7666c67464fdc4515a35dbf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
