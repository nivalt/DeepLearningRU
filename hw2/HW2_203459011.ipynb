{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpGv7oW4PwdM"
   },
   "source": [
    "# Exercise 2: Neural Networks\n",
    "\n",
    "In the previous exercise you implemented a classifier with one linear layer. In this exercise, you will implement a three layer multi-class neural network.\n",
    "\n",
    "## Submission guidelines:\n",
    "\n",
    "Your submission should only include this jupyter notebook named ex2_ID.ipynb (not in zip).\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "2. Write **efficient vectorized** code whenever possible. \n",
    "3. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "4. Do not change the functions we provided you. \n",
    "4. Write your functions in the instructed python modules only. All the logic you write is imported and used using this jupyter notebook. You are allowed to add functions as long as they are located in the python modules and are imported properly.\n",
    "5. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. Any other imports are forbidden.\n",
    "6. Your code must run without errors.\n",
    "7. **Before submitting the exercise, restart the kernel and run the notebook from start to finish to make sure everything works. You should include your desired outputs in the output cells to make your code easier to understand.**\n",
    "8. Write your own code. Cheating will not be tolerated. \n",
    "9. Answers to qualitative questions should be written in **markdown** cells (with $\\LaTeX$ support).\n",
    "\n",
    "**TIP:** You may find the following link helpful: \n",
    "http://cs231n.github.io/neural-networks-case-study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:18.152048Z",
     "start_time": "2022-11-28T06:46:17.469849Z"
    },
    "id": "pA0hjtyJPwdO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "import itertools\n",
    "\n",
    "# specify the way plots behave in jupyter notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcYX2rU2PwdP"
   },
   "source": [
    "# Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oa-bjEWl9fW6"
   },
   "source": [
    "## Data download and processing Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:18.167551Z",
     "start_time": "2022-11-28T06:46:18.153952Z"
    },
    "id": "9_uAlYcQ9dF7"
   },
   "outputs": [],
   "source": [
    "def maybe_download_and_extract(url, download_dir):\n",
    "    \"\"\"\n",
    "    Download and extract the data if it doesn't already exist.\n",
    "    Assumes the url is a tar-ball file.\n",
    "    :param url:\n",
    "        Internet URL for the tar-file to download.\n",
    "        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    :param download_dir:\n",
    "        Directory where the downloaded file is saved.\n",
    "        Example: \"data/CIFAR-10/\"\n",
    "    :return:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filename for saving the file downloaded from the internet.\n",
    "    # Use the filename from the URL and add it to the download_dir.\n",
    "    filename = url.split('/')[-1]\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    # Check if the file already exists.\n",
    "    # If it exists then we assume it has also been extracted,\n",
    "    # otherwise we need to download and extract it now.\n",
    "    if not os.path.exists(file_path):\n",
    "        # Check if the download directory exists, otherwise create it.\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "\n",
    "        # Download the file from the internet.\n",
    "        print(\"Downloading, This might take several minutes.\")\n",
    "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
    "                                                  filename=file_path)\n",
    "\n",
    "        print()\n",
    "        print(\"Download finished. Extracting files.\")\n",
    "\n",
    "        if file_path.endswith(\".zip\"):\n",
    "            # Unpack the zip-file.\n",
    "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
    "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "            # Unpack the tar-ball.\n",
    "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
    "\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(\"Data has apparently already been downloaded and unpacked.\")\n",
    "        print(\"If not, delete the dataset folder and try again.\")\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    ''' load single batch of cifar '''\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding = 'latin1')\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load(ROOT):\n",
    "    ''' load all of cifar '''\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MeVWzF19mVA"
   },
   "source": [
    "## Data Download\n",
    "\n",
    "The next cell will download and extract CIFAR-10 into `datasets/cifar10/`. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The dataset is divided into five training batches and one test batch, each with 10,000 images. The test batch contains exactly 1,000 randomly-selected images from each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:18.183294Z",
     "start_time": "2022-11-28T06:46:18.170425Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bHbknwvPwdQ",
    "outputId": "14fd8ca9-ed8b-4d8a-c12d-313a6e16d2e8"
   },
   "outputs": [],
   "source": [
    "# this cell will download the data if it does not exists\n",
    "URL = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "PATH = 'datasets/cifar10/' # the script will create required directories\n",
    "maybe_download_and_extract(URL, PATH) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beFt0AfB9w9f"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_YTIdyB5qMO"
   },
   "source": [
    "**Notice that we are leaving behind the bias trick in this exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:20.967912Z",
     "start_time": "2022-11-28T06:46:18.185329Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fyLpzTyzPwdQ",
    "outputId": "7b4b3f20-a530-4f99-8a17-9a83c7822d38"
   },
   "outputs": [],
   "source": [
    "CIFAR10_PATH = os.path.join(PATH, 'cifar-10-batches-py')\n",
    "X_train, y_train, X_test, y_test = load(CIFAR10_PATH) # load the entire data\n",
    "num_classes = 4\n",
    "\n",
    "X_train = X_train[np.isin(y_train, range(num_classes))]\n",
    "y_train = y_train[np.isin(y_train, range(num_classes))]\n",
    "X_test = X_test[np.isin(y_test, range(num_classes))]\n",
    "y_test = y_test[np.isin(y_test, range(num_classes))]\n",
    "\n",
    "# define a splitting for the data\n",
    "num_training = num_classes*5000\n",
    "num_validation = 1000\n",
    "num_testing = 1000\n",
    "\n",
    "# add a validation dataset for hyperparameter optimization\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "mask = range(num_validation)\n",
    "X_val = X_test[mask]\n",
    "y_val = y_test[mask]\n",
    "mask = range(num_validation, num_validation+num_testing)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# float64\n",
    "X_train = X_train.astype(np.float64)\n",
    "X_val = X_val.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "# subtract the mean from all the images in the batch\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "\n",
    "# flatten all the images in the batch (make sure you understand why this is needed)\n",
    "X_train = np.reshape(X_train, newshape=(X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, newshape=(X_val.shape[0], -1)) \n",
    "X_test = np.reshape(X_test, newshape=(X_test.shape[0], -1)) \n",
    "\n",
    "print(f\"Shape of training set: {X_train.shape}\")\n",
    "print(f\"Shape of validation set: {X_val.shape}\")\n",
    "print(f\"Shape of test set: {X_test.shape}\")\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.137947Z",
     "start_time": "2022-11-28T06:46:20.972016Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "mHATPoNJPwdQ",
    "outputId": "bc544bf9-ecce-4903-eeeb-22afee3b745c"
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, n=1000):\n",
    "    rand_items = np.random.randint(0, X.shape[0], size=n)\n",
    "    images = X[rand_items]\n",
    "    labels = y[rand_items]\n",
    "    return images, labels\n",
    "\n",
    "def make_random_grid(x, y, n=4, convert_to_image=True, random_flag=True):\n",
    "    if random_flag:\n",
    "        rand_items = np.random.randint(0, x.shape[0], size=n)\n",
    "    else:\n",
    "        rand_items = np.arange(0, x.shape[0])\n",
    "    images = x[rand_items]\n",
    "    labels = y[rand_items]\n",
    "    if convert_to_image:\n",
    "        grid = np.hstack(np.array([np.asarray((vec_2_img(i) + mean_image), dtype=np.int64) for i in images]))\n",
    "    else:\n",
    "        grid = np.hstack(np.array([np.asarray(i, dtype=np.int64) for i in images]))\n",
    "    print(' '.join('%9s' % classes[labels[j]] for j in range(n)))\n",
    "    return grid\n",
    "\n",
    "def vec_2_img(x):\n",
    "    x = np.reshape(x, (32, 32, 3))\n",
    "    return x\n",
    "\n",
    "X_batch, y_batch = get_batch(X_test, y_test, 100)\n",
    "plt.imshow(make_random_grid(X_batch, y_batch, convert_to_image=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9MRQD6v4aDz"
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRuXmTuzPwdS"
   },
   "source": [
    "## Cross-entropy\n",
    "\n",
    "\n",
    "Complete the function `softmax_loss` using vectorized code. This function takes as input `scores`, labels `y` and outputs the calculated loss as a single number and the gradients with respect to X. **(10 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.153755Z",
     "start_time": "2022-11-28T06:46:21.140549Z"
    },
    "id": "GQ2BtZDMOaMv"
   },
   "outputs": [],
   "source": [
    "def softmax_loss(scores, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - scores: scores of shape (N, C) where scores[i, c] is the score for class c on input X[i].\n",
    "    - y: Vector of labels\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement this function                                           #\n",
    "    ###########################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########################################################################\n",
    "    #                              END OF YOUR CODE                           #\n",
    "    ###########################################################################\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.169038Z",
     "start_time": "2022-11-28T06:46:21.155754Z"
    },
    "id": "GQ2BtZDMOaMv"
   },
   "outputs": [],
   "source": [
    "# some tests\n",
    "np.random.seed(42)\n",
    "\n",
    "num_instances = 5\n",
    "num_classes = 3\n",
    "\n",
    "y = np.random.randint(num_classes, size=num_instances)\n",
    "scores = np.random.randn(num_instances * num_classes).reshape(num_instances, num_classes)\n",
    "loss, dx = softmax_loss(scores, y)\n",
    "\n",
    "\n",
    "correct_grad = np.array([[ 0.0062,  0.1751, -0.1813],\n",
    "         [-0.1463,  0.0561,  0.0901],\n",
    "         [ 0.0404,  0.0771, -0.1174],\n",
    "         [ 0.0223,  0.0855, -0.1078],\n",
    "         [-0.1935,  0.1358,  0.0578]])\n",
    "correct_loss = 1.7544\n",
    "assert np.isclose(dx.round(4), correct_grad, rtol=1e-3).all()\n",
    "assert np.isclose(loss.round(4), correct_loss, rtol=1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3b1r34XiywH"
   },
   "source": [
    "## L2 Regularization\n",
    "\n",
    "Regularization is a very important technique in machine learning to prevent overfitting. Mathematically speaking, it adds a regularization term to the loss to penalize larger weights. \n",
    "$$\n",
    "Loss = Loss + \\lambda  \\cdot \\frac{1}{2} \\cdot \\sum_{i=0}^k w_k^2\n",
    "$$\n",
    "\n",
    "Implement the L2 regularization part of the loss in the next cell: **(10 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.184401Z",
     "start_time": "2022-11-28T06:46:21.171489Z"
    },
    "id": "wk--K9pGi_Lb"
   },
   "outputs": [],
   "source": [
    "def l2_regulariztion_loss(W, reg=0):\n",
    "    \"\"\"\n",
    "    L2 regulariztion loss function, vectorized version.\n",
    "    - W: a layer's weights.\n",
    "    - reg: (float) regularization strength\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    #############################################################################\n",
    "    # TODO: Compute the L2 reulariztion loss and its gradient using no \n",
    "    # explicit loops.                                                           #\n",
    "    # Store the loss in loss and the gradient in dW.                            #\n",
    "    #############################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #############################################################################\n",
    "    #                          END OF YOUR CODE                                 #\n",
    "    #############################################################################\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xerXQe6hPwdU"
   },
   "source": [
    "# Neural Network\n",
    "\n",
    "The implementation of linear regression was (hopefully) simple yet not very modular since the layer, loss and gradient were calculated as a single monolithic function. This would become impractical as we move towards bigger models. As a warmup towards `PyTorch`, we want to build networks using a more modular design so that we can implement different layer types in isolation and easily integrate them together into models with different architectures.\n",
    "\n",
    "This logic of isolation & integration is at the heart of all popular deep learning frameworks, and is based on two methods each layer holds - a forward and backward pass. The forward function will receive inputs, weights and other parameters and will return both an output and a cache object storing data needed for the backward pass. The backward pass will receive upstream derivatives and the cache, and will return gradients with respect to the inputs and weights. By implementing several types of layers this way, we will be able to easily combine them to build classifiers with different architectures with relative ease.\n",
    "\n",
    "We will implement a neural network to obtain better results on CIFAR-10. \n",
    "Our neural network will be implemented in the following cells. We will train this network using softmax loss and L2 regularization and a ReLU non-linearity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHH9dGgt4NP3"
   },
   "source": [
    "### Fully Connected Layer: Forward Pass. \n",
    "\n",
    "Implement the function `fc_forward`. **(5 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.200656Z",
     "start_time": "2022-11-28T06:46:21.189432Z"
    },
    "id": "OO3vgLtGVWkv"
   },
   "outputs": [],
   "source": [
    "def fc_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an fully connected layer.\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - W: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "    # will need to reshape the input into rows.                                 #\n",
    "    #############################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = (X.copy(), W.copy(), b.copy())\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.215718Z",
     "start_time": "2022-11-28T06:46:21.202326Z"
    },
    "id": "bkwG7GJ_PwdU"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_instances = 5\n",
    "input_shape = (11, 7, 3)\n",
    "output_shape = 4\n",
    "\n",
    "X = np.random.randn(num_instances * np.prod(input_shape)).reshape(num_instances, *input_shape)\n",
    "W = np.random.randn(np.prod(input_shape) * output_shape).reshape(np.prod(input_shape), output_shape)\n",
    "b = np.random.randn(output_shape)\n",
    "\n",
    "out, _ = fc_forward(X, W, b)\n",
    "\n",
    "correct_out = np.array([[16.77132953,  1.43667172, -15.60205534,   7.15789287],\n",
    "                        [ -8.5994206,  7.59104298,  10.92160126,  17.19394331],\n",
    "                        [ 4.77874003,  2.25606192,  -6.10944859,  14.76954561],\n",
    "                        [21.21222953, 17.82329258,   4.53431782,  -9.88327913],\n",
    "                        [18.83041801, -2.55273817,  14.08484003,  -3.99196171]])\n",
    "\n",
    "assert np.isclose(out, correct_out, rtol=1e-8).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaJgjBa2PwdU"
   },
   "source": [
    "### Fully Connected Layer: Backward Pass \n",
    "\n",
    "Implement the function `fc_backward` **(5 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.231622Z",
     "start_time": "2022-11-28T06:46:21.217581Z"
    },
    "id": "GKGLxK7wVakI"
   },
   "outputs": [],
   "source": [
    "def fc_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an fully connected layer.\n",
    "    Try the link in the exercise intructions for more details.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives\n",
    "    - cache: Tuple of:\n",
    "      - X: Input data\n",
    "      - W: Weights\n",
    "      - b: Biases\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to X\n",
    "    - dw: Gradient with respect to W\n",
    "    - db: Gradient with respect to b\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = 0, 0, 0\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine backward pass.                               #\n",
    "    ###########################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.247332Z",
     "start_time": "2022-11-28T06:46:21.232614Z"
    },
    "id": "K3Xcoeqxnq_F"
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.262753Z",
     "start_time": "2022-11-28T06:46:21.248711Z"
    },
    "id": "7J13imMzPwdU"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: fc_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: fc_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: fc_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "out, cache = fc_forward(x,w,b)\n",
    "dx, dw, db = fc_backward(dout, cache)\n",
    "\n",
    "assert np.isclose(dw, dw_num, rtol=1e-8).all() # simple test\n",
    "assert np.isclose(dx, dx_num, rtol=1e-8).all() # simple test\n",
    "assert np.isclose(db, db_num, rtol=1e-8).all() # simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6D5b-J5PwdV"
   },
   "source": [
    "### ReLU: Forward Pass \n",
    "\n",
    "Implement the function `relu_forward`. **(5 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.278815Z",
     "start_time": "2022-11-28T06:46:21.264744Z"
    },
    "id": "bO5iMs3aVeTl"
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                    #\n",
    "    #############################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = x.copy()\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.294685Z",
     "start_time": "2022-11-28T06:46:21.280962Z"
    },
    "id": "WdYx8zaOPwdV"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "assert np.isclose(out, correct_out, rtol=1e-8).all() # simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOP-MRufPwdV"
   },
   "source": [
    "### ReLU: Backward Pass\n",
    "\n",
    "Implement the function `relu_backward`. **(5 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.310754Z",
     "start_time": "2022-11-28T06:46:21.296684Z"
    },
    "id": "n3YsNEphVhuo"
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                   #\n",
    "    #############################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.325592Z",
     "start_time": "2022-11-28T06:46:21.312932Z"
    },
    "id": "w_9OFZZ-PwdV"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "xx, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "assert np.isclose(dx, dx_num, rtol=1e-8).all()  # simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kX0UZJdDPwdV"
   },
   "source": [
    "### Combined Layer\n",
    "Next combine the fully connected and relu forward\\backward functions togther using the functions in the following cell. \n",
    "Remember to use functions you already implemented.\n",
    "**(5 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.341148Z",
     "start_time": "2022-11-28T06:46:21.327148Z"
    },
    "id": "yhSV6tHgpZd0"
   },
   "outputs": [],
   "source": [
    "def fc_relu_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Forward pass for a fully connected layer followed by a ReLU.\n",
    "\n",
    "    Inputs:\n",
    "    - X: Input to the fc layer\n",
    "    - W, b: Weights for the fc layer\n",
    "\n",
    "    Returns:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    #############################################################################\n",
    "    # TODO: Implement the function.                                             #\n",
    "    #############################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = (fc_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def fc_relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for a fully connected layer followed by a ReLU\n",
    "    Inputs:\n",
    "    - dout: upstream derivatives\n",
    "    - cache: parameters calculated during the forward pass\n",
    "\n",
    "    Returns:\n",
    "    - dX: derivative w.r.t X\n",
    "    - dW: derivative w.r.t W\n",
    "    - db: derivative w.r.t b\n",
    "    \"\"\"\n",
    "    fc_cache, relu_cache = cache\n",
    "    #############################################################################\n",
    "    # TODO: Implement the function.                                             #\n",
    "    #############################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RNFsQVGrFUE"
   },
   "source": [
    "You can check your results in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.357151Z",
     "start_time": "2022-11-28T06:46:21.343147Z"
    },
    "id": "wfsIy8dEqx7r"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: fc_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: fc_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: fc_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "out, cache = fc_relu_forward(x,w,b)\n",
    "dx, dw, db = fc_relu_backward(dout, cache)\n",
    "\n",
    "assert np.isclose(dw, dw_num, rtol=1e-8).all() # simple test\n",
    "assert np.isclose(dx, dx_num, rtol=1e-8).all() # simple test\n",
    "assert np.isclose(db, db_num, rtol=1e-8).all() # simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHYeDvNcPwdV"
   },
   "source": [
    "# Building the Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7-k0EkePwdV"
   },
   "source": [
    "Complete the class `ThreeLayerNet`. **(35 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.388140Z",
     "start_time": "2022-11-28T06:46:21.358811Z"
    },
    "id": "883fce5uWDVl"
   },
   "outputs": [],
   "source": [
    "class ThreeLayerNet(object):\n",
    "    \"\"\"\n",
    "    A three-layer fully-connected neural network. This network has an input dimension of\n",
    "    N, a hidden layer dimension of H, and performs classification over C classes.\n",
    "    In our case, we use the same hidden dimension across all hidden layers.\n",
    "    We train the network with a softmax loss function and L2 regularization on the\n",
    "    weight matrices. In other words, the network has the following architecture:\n",
    "\n",
    "    input - fc layer - ReLU - fc layer - ReLu - fc layer - softmax\n",
    "\n",
    "    The outputs of the third fully-connected layer are the scores for each class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-2):\n",
    "        \"\"\"\n",
    "        Initialize the model. Weights are initialized to small random values and\n",
    "        biases are initialized to zero. Weights and biases are stored in the\n",
    "        variable self.params, which is a dictionary with the following keys:\n",
    "\n",
    "        W1: First layer weights; has shape (D, H)\n",
    "        b1: First layer biases; has shape (H,)\n",
    "        W2: Second layer weights; has shape (H, H)\n",
    "        b2: Second layer biases; has shape (H,)\n",
    "        W3: Second layer weights; has shape (H, C)\n",
    "        b3: Second layer biases; has shape (C,)\n",
    "\n",
    "        Inputs:\n",
    "        - input_size: The dimension D of the input data.\n",
    "        - hidden_size: The number of neurons H in each of the hidden layers.\n",
    "        - output_size: The number of classes C.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "    def step(self, X, y=None, reg=0.0):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients for a three layer fully connected neural\n",
    "        network.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "        - y: Vector of training labels. This parameter is optional; if it\n",
    "          is not passed then we only return scores, and if it is passed then we\n",
    "          instead return the loss and gradients.\n",
    "        - reg: Regularization coefficient.\n",
    "\n",
    "        Returns:\n",
    "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "        the score for class c on input X[i].\n",
    "\n",
    "        If y is not None, instead return a tuple of:\n",
    "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "          samples.\n",
    "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "          with respect to the loss function; has the same keys as self.params.\n",
    "        \"\"\"\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W3, b3 = self.params['W3'], self.params['b3'] \n",
    "\n",
    "        # Compute the forward pass\n",
    "        scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Perform the forward pass, computing the class scores for the input. #\n",
    "        # Store the result in the scores variable, which should be an array of      #\n",
    "        # shape (N, C).                                                             #\n",
    "        #############################################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        #                              END OF YOUR CODE                             #\n",
    "        #############################################################################\n",
    "\n",
    "        # If the targets are not given then jump out, we're done\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = None\n",
    "        ###############################################################################\n",
    "        # After you finished the forward pass, compute the loss. This should include  #\n",
    "        # both the data loss and L2 regularization for W1, W2, W3. Store the result   #\n",
    "        # in the variable loss, which should be a scalar. Use the softmax_loss        #\n",
    "        # and l2_regulariztion_loss functions you implemented.                        #         \n",
    "        ###############################################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        #                              END OF YOUR CODE                             #\n",
    "        #############################################################################\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        grads = {}\n",
    "        #############################################################################\n",
    "        # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
    "        # and biases. Store the results in the grads dictionary. For example,       #\n",
    "        # grads['W1'] = dW1 + dW1_reg, it stores the gradient on W1, including      #\n",
    "        # regularization. It should be a matrix of the same size.                   #\n",
    "        #############################################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        #                              END OF YOUR CODE                             #\n",
    "        #############################################################################\n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, X, y, X_val, y_val,\n",
    "              learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "              batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this neural network using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving training data.\n",
    "        - y: A numpy array f shape (N,) giving training label.\n",
    "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "        - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "        - learning_rate: Scalar giving learning rate for optimization.\n",
    "        - reg: Scalar giving regularization strength.\n",
    "        - num_iters: Number of steps to take when optimizing.\n",
    "        - batch_size: Number of training examples to use per step.\n",
    "        - verbose: boolean; if true print progress during optimization.\n",
    "        \"\"\"\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "        # Use SGD to optimize the parameters in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "            #########################################################################\n",
    "            # TODO: Create a random minibatch of training data and labels, storing  #\n",
    "            # them in X_batch and y_batch respectively.                             #\n",
    "            #########################################################################\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            #########################################################################\n",
    "            #                             END OF YOUR CODE                          #\n",
    "            #########################################################################\n",
    "\n",
    "            # Compute loss and gradients using the current minibatch\n",
    "            loss, grads = self.step(X_batch, y=y_batch, reg=reg)\n",
    "            loss_history.append(loss)\n",
    "            #########################################################################\n",
    "            # TODO: Use the gradients in the grads dictionary to update the         #\n",
    "            # parameters of the network (stored in the dictionary self.params)      #\n",
    "            # using stochastic gradient descent. You'll need to use the gradients   #\n",
    "            # stored in the grads dictionary defined above.                         #\n",
    "            #########################################################################\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #########################################################################\n",
    "            #                             END OF YOUR CODE                          #\n",
    "            #########################################################################\n",
    "\n",
    "            if verbose and (it+1) % 100 == 0:\n",
    "                print ('iteration %d / %d: loss %f' % (it+1, num_iters, loss))\n",
    "\n",
    "            # Every epoch, check train and val accuracy.\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "\n",
    "        return {\n",
    "          'loss_history': loss_history,\n",
    "          'train_acc_history': train_acc_history,\n",
    "          'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this three-layer network to predict labels for\n",
    "        data points. For each data point we predict scores for each of the C\n",
    "        classes, and assign each data point to the class with the highest score.\n",
    "\n",
    "        Inputs:\n",
    "        - X: data points to classify.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: predicted labels\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W3, b3 = self.params['W3'], self.params['b3'] \n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Implement this function                                           #\n",
    "        ###########################################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ###########################################################################\n",
    "        #                              END OF YOUR CODE                           #\n",
    "        ###########################################################################\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.403237Z",
     "start_time": "2022-11-28T06:46:21.390103Z"
    },
    "id": "i6KXQ1KvPwdV"
   },
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 128\n",
    "num_classes = 4\n",
    "model = ThreeLayerNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:47:07.708024Z",
     "start_time": "2022-11-28T06:46:21.405440Z"
    },
    "id": "XbUwlaa9PwdV"
   },
   "outputs": [],
   "source": [
    "stats = model.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1500, batch_size=200,\n",
    "            learning_rate=1e-3, reg=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:47:07.754799Z",
     "start_time": "2022-11-28T06:47:07.711049Z"
    },
    "id": "-1hDhsb1PwdV"
   },
   "outputs": [],
   "source": [
    "val_acc = (model.predict(X_val) == y_val).mean()\n",
    "print ('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:47:08.208742Z",
     "start_time": "2022-11-28T06:47:07.756434Z"
    },
    "id": "VdhExUOrKcc6"
   },
   "outputs": [],
   "source": [
    "train_acc = (model.predict(X_train) == y_train).mean()\n",
    "print ('Training accuracy: ', train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:47:08.619195Z",
     "start_time": "2022-11-28T06:47:08.210823Z"
    },
    "id": "F-9rpmQAPwdW"
   },
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Okkhr5xjPwdW"
   },
   "source": [
    "## Hyperparameter Optimization\n",
    "Use the validation set to tune hyperparameters by training different models (using the training dataset) and evaluating the performance using the validation dataset. Save the results in a dictionary mapping tuples of the form `(learning_rate, hidden_size, regularization)` to tuples of the form `(training_accuracy, validation_accuracy)`. You should evaluate the best model on the testing dataset and print out the training, validation and testing accuracies for each of the models and provide a clear visualization. Highlight the best model w.r.t the testing accuracy. **(10 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T07:02:08.693145Z",
     "start_time": "2022-11-28T06:47:08.623196Z"
    },
    "id": "WU33Q_kwPwdW"
   },
   "outputs": [],
   "source": [
    "# This might take some time, try to expirement with small number of testing parameters before continuing\n",
    "# You are encouraged to experiment with additional values\n",
    "learning_rates = [1e-4, 1e-3]\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "regularizations = [0, 0.001, 0.1, 0.25] \n",
    "\n",
    "results = {}\n",
    "best_val = -1   \n",
    "best_net = None \n",
    "################################################################################\n",
    "#                            START OF YOUR CODE                                #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "\n",
    "# Print out results.\n",
    "for lr, hidden_size, reg  in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, hidden_size, reg)]\n",
    "    print ('lr %e hidden_size %f reg %f train accuracy: %f val accuracy: %f' % (\n",
    "                lr, hidden_size, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print ('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "test_accuracy = (model.predict(X_test) == y_test).mean()\n",
    "print ('Neural Network on raw pixels final test set accuracy: %f' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkyH1MtdPwdW"
   },
   "source": [
    "# Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkyH1MtdPwdW"
   },
   "source": [
    "##**Question:** \n",
    "What can you say about the training? Why does it take much longer to train (compare to hw1)? **(5 Points)**\n",
    "\n",
    "**Your answer:** *Fill this in*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkyH1MtdPwdW"
   },
   "source": [
    "##**Question:** \n",
    "\n",
    "What can you say about the diffrence (or lack of thereof) between the validation and training accuracy? What can you say about the connection between the loss and the accuracy? **(5 Points)**\n",
    "\n",
    "**Your answer:** *Fill this in*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh8-pdw3-3u7"
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Oa-bjEWl9fW6",
    "8MeVWzF19mVA"
   ],
   "name": "HW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
